{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpYK9fIlyZCq"
      },
      "source": [
        "# Lab Deep Learning / Multi-Layer Perceptron for classification / in pytorch\n",
        "\n",
        "**Author: geoffroy.peeters@telecom-paris.fr**\n",
        "\n",
        "For any remark or suggestion, please feel free to contact me.\n",
        "\n",
        "Last edit: 2020/04/27 peeters\n",
        "\n",
        "Read the \"introduction to pytorch\" slides first: https://perso.telecom-paristech.fr/gpeeters/doc/pytorch/\n",
        "\n",
        "**Is is recommended that you first perform the ```Lab Deep Learning / Multi-Layer Perceptron for regression / in pytorch``` Lab to understand how to write a Neural Network in pytorch.**\n",
        "\n",
        "\n",
        "## Objective:\n",
        "\n",
        "The objective of this lab is to develop a two hidden layers MLP to perform image **classification**.\n",
        "\n",
        "While the previous Lab ```Lab Deep Learning / Multi-Layer Perceptron for regression / in pytorch``` focused on how to write a Neural Network, the present Lab focuses on managing datasets and splitting training and testing .\n",
        "\n",
        "We will use MNIST for the image dataset.\n",
        "\n",
        "## Your task:\n",
        "\n",
        "You need to add the missing parts in the code (parts between ```# --- START CODE HERE``` and ```# --- END CODE HERE```)\n",
        "\n",
        "## Documentation:\n",
        "- NN: https://pytorch.org/docs/stable/nn.html\n",
        "- Autograd: https://pytorch.org/docs/stable/autograd.html\n",
        "- Optim: https://pytorch.org/docs/stable/optim.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "h6o3NZfNyZCt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy7u-IVkyZCu"
      },
      "source": [
        "## Data management\n",
        "\n",
        "It is common to separate the datasets into\n",
        "- a **training** part (used to find the network parameters which minimize the loss)\n",
        "- a **test part** (used to estimate the performances).\n",
        "\n",
        "The training part can itself be further splitted into a training and a validation part (the validation part is then used to fix the hyper-parameters of the system).\n",
        "\n",
        "When performing ```batch gradient descent``` the whole training set is used to compute the gradients (which itself is used for the parameter optimization).\n",
        "However when performing ```stochastic or mini-batch gradient descent``` only a part of the training data is used at each iteration.\n",
        "\n",
        "We therefore needs to write all the necessary code, to get these mini-batches of data, be sure that all data have been seen, potentially randomize the order of appearance of these data and potentially perform some modifications of the data before giving them to the network.\n",
        "\n",
        "Fortunatelly, pytorch has associated to it (but not included in it) a nice package (```torchvision```) which allows to do all that for us.\n",
        "\n",
        "This package allows to perform these process for any datasets but has a pre-tuned method for the ```MNIST```datasets.\n",
        "\n",
        "### Datasets\n",
        "\n",
        "In the following we will us ```datasets.MNIST``` to load the train and test data.\n",
        "\n",
        "We need to tell it to download the data, where to dowload them and if we want the training or the testing part of it.\n",
        "\n",
        "We then tell it if we want to apply some transformations to the data.\n",
        "\n",
        "For this lab, we will transform the data by\n",
        "- 1) transforming them to torch tensors (using ```transforms.ToTensor()```); since by default they are not torch tensors\n",
        "- 2) normalize them to zero mean and unit standard deviation (using ```transforms.Normalize()```).\n",
        "\n",
        "It is of course possible to also apply data augmentation.\n",
        "\n",
        "Since we have several transforms to be performed, we will compose them using ```transforms.Compose([transform1, transform2])```\n",
        "\n",
        "The corresponding datasets is a class which contains the pairs of [input tensors $X$, corresponding ground-truth label $y$].\n",
        "\n",
        "### Dataloader\n",
        "\n",
        "Another nice pytorch facility is the ```Dataloader``` which transforms the dataset to be used as an iterator (i.e. within ```for data in myDataloader: ```).\n",
        "\n",
        "Moreover, ```Dataloader``` allows to specify\n",
        "- how many data will be provided each time it is called (```batch_size```)\n",
        "- if the order of appearance of the data with be random (````shuffle````).\n",
        "\n",
        "We therefore convert our datasets (train and test) to ```DataLoader``` that are directly used as iterator during the iterations (it DataLoader manage directly the mini-batch and shuffling)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zsSEvakPyZCv"
      },
      "outputs": [],
      "source": [
        "# --- START CODE HERE (01)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=(0,), std=(1,))])\n",
        "\n",
        "train_set = datasets.MNIST(root='./', transform=transform, train=True, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=BATCH_SIZE)\n",
        "test_set = datasets.MNIST(root='./', transform=transform, train=False, download=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, shuffle=False, batch_size=BATCH_SIZE)\n",
        "\n",
        "# --- END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "FFuEXr0tyZCw",
        "outputId": "59ad5329-e7b3-4dd3-f0f7-79bbbcfde7f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset type: <class 'torchvision.datasets.mnist.MNIST'>\n",
            "dataset size: 60000\n",
            "X data: torch.Size([1, 28, 28])\n",
            "y data: 5\n"
          ]
        }
      ],
      "source": [
        "print(\"dataset type: {}\".format(type(train_set)))\n",
        "print(\"dataset size: {}\".format(len(train_set)))\n",
        "print(\"X data: {}\".format(train_set[0][0].size()))\n",
        "print(\"y data: {}\".format(train_set[0][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piAf25dxyZCx"
      },
      "source": [
        "### Display the first 5 data of MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Jsa2MAEhyZCx"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAACXCAYAAAC1ITlNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAe80lEQVR4nO3deVhU1/kH8BfQGYgKiAqKglLRGsUtFhW3xzQ+MZoYTXBJl0eNe4IaXOoSl/weqyHRVE0JidHWtVqstmpijI1BxccEsNiYRIlLmsQNwWjLDG6AcH9/JJy8F2dgtnvvmZnv53nmed65c5k5zMu5Hs+555wARVEUAgAAANBJoNEFAAAAAP+CxgcAAADoCo0PAAAA0BUaHwAAAKArND4AAABAV2h8AAAAgK7Q+AAAAABdofEBAAAAukLjAwAAAHSFxgcAAADoCo0PF5SVldH8+fMpOjqaQkJCqFevXnTo0CGji+X3jh49SgEBATYfubm5RhcPmBUrVlBAQAAlJCQYXRS/hjrjPXytztQzugDeaPz48bR7925KTU2ldu3a0ebNm2no0KF05MgR6tevn9HF83szZ86kxMRE1bH4+HiDSgM1XblyhV599VVq0KCB0UWBH6HOyM0X6wwaH046ceIEZWZm0qpVq2ju3LlERDR27FhKSEigefPm0aeffmpwCaF///40cuRIo4sBdsydO5d69+5NlZWVdOPGDaOLA4Q6IztfrDMYdnHS7t27KSgoiKZMmSKOBQcH08SJEyknJ4cuX75sYOmgWmlpKd2/f9/oYkANx44do927d9PatWuNLgrUgDojJ1+tM2h8OOmzzz6j9u3bU2hoqOp4z549iYjo1KlTBpQKuOeff55CQ0MpODiYHn30UcrPzze6SEBElZWVNGPGDJo0aRJ17tzZ6OIAgzojJ1+uMxh2cdK1a9eoRYsWDxyvPlZYWKh3keBHJpOJkpOTaejQodS0aVMqKCigN954g/r370+ffvopde/e3egi+rV169bRxYsX6eOPPza6KPAj1Bm5+XKdQePDSXfv3iWz2fzA8eDgYPE6GKNPnz7Up08f8fzpp5+mkSNHUpcuXWjhwoV08OBBA0vn327evElLly6lJUuWULNmzYwuDvwIdUZevl5nMOzipJCQECorK3vg+L1798TrII/4+HgaPnw4HTlyhCorK40ujt9avHgxRURE0IwZM4wuCtQBdUYOvl5n0PPhpBYtWtDVq1cfOH7t2jUiIoqOjta7SFCHmJgYKi8vp9u3bz9wrw5o78KFC7R+/Xpau3ataljy3r17VFFRQd999x2FhoZSRESEgaUEDnXGWP5QZ9Dz4aRu3brR+fPnyWq1qo7n5eWJ10Eu33zzDQUHB1PDhg2NLopfunr1KlVVVdHMmTMpLi5OPPLy8uj8+fMUFxdHy5YtM7qYwKDOGMsf6kyAoiiK0YXwJnl5edS7d2/VOh9lZWWUkJBATZo0waqABvr+++8fGBv9/PPPKTExkYYMGUL79u0zqGT+7caNG3T8+PEHji9evJhKS0vpzTffpLZt2/rc3fzeAHVGTv5QZ9D4cMHo0aNpz549NGvWLIqPj6ctW7bQiRMnKCsriwYMGGB08fzWL3/5SwoJCaE+ffpQZGQkFRQU0Pr166l+/fqUk5NDDz/8sNFFBGbgwIF048YNOn36tNFF8VuoM97Fl+oM7vlwwdatW2nJkiW0bds2+t///kddunSh/fv3o+FhsBEjRtD27dtp9erVZLVaqVmzZvTss8/SK6+8gqWiAWxAnQGjoOcDAAAAdIUbTgEAAEBXaHwAAACArtD4AAAAAF2h8QEAAAC60qzxkZGRQW3atKHg4GDq1asXnThxQquPAicgL/JCbuSF3MgJefFiigYyMzMVk8mkbNy4UTlz5owyefJkJTw8XCkuLtbi48BByIu8kBt5ITdyQl68myZTbXv16kWJiYn01ltvERFRVVUVxcTE0IwZM2jBggW1/mxVVRUVFhZSo0aNKCAgwNNF81uKotDAgQOpT58+lJGRQUTO5aX6fOTGsxRFodLSUkpOTna5zlSfj9x4lidyg7xoA9czOVXXmejoaAoMrH1gxeOLjJWXl9PJkydp4cKF4lhgYCANGjSIcnJyHji/rKxMtUvs1atXqWPHjp4uFvwoJSVFxLXlhQi50VNQUJDDdYYIudGTM7lBXvSF65mcLl++TK1atar1HI/f83Hjxg2qrKykqKgo1fGoqCgqKip64Py0tDQKCwsTD/wxaKt169aq5/byQoTc6MmZOkOE3OgJ1zN54Xomp0aNGtV5juGzXRYuXEgWi0U8Ll++bHSRfJoz3YvIjbyQGzkhL/rC9UxOjuTF48MuTZs2paCgICouLlYdLy4upubNmz9wvtlsJrPZ7OligB3Xr19XPbeXFyLkRk/O1Bki5EZPuJ7JC9cz7+Xxng+TyUQ9evSgrKwscayqqoqysrIoKSnJ0x8HTsrOzhYx8iKPbt26oc5ICrmRF65nXkyLKTSZmZmK2WxWNm/erBQUFChTpkxRwsPDlaKiojp/1mKxKESEh0YPV/OC3Gj72LhxI3Ij6cOd3CAv2j5QZ+R8WCyWOr9/TRofiqIo6enpSmxsrGIymZSePXsqubm5Dv0c/iC0faxatcqlvCA32j4sFovLdQa5kTc3yIu2D1zP5Hw40vjQZJ0Pd1itVgoLCzO6GD7LYrFQaGioSz+L3GjHnbwQITdaQp2RF3IjJ0fy4vEbTgEAAKo1adJE9bxDhw42z/vkk0/0KA5IwvCptgAAAOBf0PgAAAAAXWHYxUCzZs0S8ZIlS0RcUlKiOu+JJ54Q8fnz5zUvl7fh3bq7du0S8T//+U8Rr1y5UsSO3OYUHh4u4u7du4v4N7/5jeq8ffv2iZivL7Bhw4Y6P8OfxcTEiDg1NVXEmzZtEvHp06f1LBK4acSIESLme6tER0erzmvZsqWI9+/fL+Lhw4drVziQDno+AAAAQFdofAAAAICuMOyiET4UwLuSu3btKmK+6x8fCqg5RalLly4ixrDLgx5//HERP/rooyJu2rSpiN944w0R8xyMGjVKxE899ZSI+cZINTev4iZOnChii8Ui4m3bton43r17tf8CfujQoUMijo+PFzHvkn/uued0LRM4hl+30tPTRcyHTRxdwSE4ONhzBQOH7N69W8SVlZWq18aMGaNbOdDzAQAAALpC4wMAAAB0hWEXD3rsscdEvHr1ahF36tTJiOL4ND4Uxb9r7k9/+pOIk5OTRbxu3ToRN27cuM7PKi8vF/GOHTtUr3300Uc2f+b+/ft1vq+/ad++vYjbtWsnYskWWYYf8WEUvllb7969Rdy/f38R3759W8QXLlwQ8V//+lfV+27fvt3mz4M+2rZtK+J69dRNgPr164u4oqJC03Kg5wMAAAB0hcYHAAAA6ArDLm7iMyTmz58vYneGWmrOaMnNzXX5vXzV8uXLRcwX9+LefPNNm8f5Im5vvfWWiA8cOGDz/IKCAhFfvHjRmWICwxfSA2PxRfSeeeYZEfO69PLLL4v4oYceqvM9x48fL+K///3vDpVj586dDp3n68xms4j5MK+nhiT5zL+IiAgRl5WVqc4LCAjwyOc5Aj0fAAAAoCs0PgAAAEBXaHwAAACArnDPhwv4FDS+KRafjuaOmzdvqp5fuXLFI+/r7fjYNF/V1BFHjx61+T41N/EDz+rWrZuIn376aREHBv70/56qqio9i+RX+Eq9X375pYhjY2Ntns/H/Pm9B7///e9FzFfIxOZ/ruvQoYOI+Yq/kydPFvHBgwc98ll8ejRfoZZvxEmkzrnW0PMBAAAAukLjAwAAAHSFYZdaREdHi/iDDz4QMV9d053uY3Q91y0qKkrEr732moj51LQ7d+6ImK92yrsQ16xZI+Jbt255vJzwE97Vv3jxYhE3aNBAxPzvHSucehbvVt+zZ4+IY2JiRMy/8y+++ELEfDr02bNnRfz11197vJz+jn/X/N+a2jaydBXfcFMWTvd8HDt2jIYNG0bR0dEUEBBAe/fuVb2uKAotXbqUWrRoQSEhITRo0CDVUrtgrPbt2yMvElqxYgXqjKRQZ+SF3Hgvpxsft2/fpq5du1JGRobN11euXEl//OMfad26dZSXl0cNGjSgwYMHY1txSaxZswZ5kdC7776LOiMp1Bl5ITfey+lhlyFDhtCQIUNsvqYoCq1du5YWL14sZoRs3bqVoqKiaO/evfTcc8+5V1od8K5Jfidw586dRcy7LO11H/MZK4cPH7b5WaNHj7b5s1p68sknKTQ01GvyMmHCBBHzjcn4UMvIkSNF/OGHH+pTMA+bO3eu19aZmvjMiBEjRtg8h/8vNT4+XusiucUb6gzvqt+2bZuIH3nkEREXFhaKePbs2SLmuTh16pRGJdSGN+SG4zPt+Owvruaqo54wbNgwm8e/+uorj3+Wozx6w+m3335LRUVFNGjQIHEsLCyMevXqRTk5OTZ/pqysjKxWq+oB2qsrL0TIjZ4GDhwoYuRGTsiLvJAb7+PRxkdRURERqW8SrH5e/VpNaWlpFBYWJh685wG0VVteiJAbPUVGRqqeIzdyQl7khdx4F8NnuyxcuFDVBWi1Wg39o+Bddr/4xS+c+lk+vDJr1iwRnzlzRsQvvfSSiPmwi4yMyg2fFTFp0iSb5yxdulTE3jrU4g7Z6g2/W3/ixIk2z9mwYYOI+bAZrxPeTq+81Byq4pso9uvXT8RXr14V8cyZM0XMZ8H4C6PqDF/gi2+IyTfrO378uIj5sJk7Ro0aJWL+e/Ien3feeccjn+UKjzY+qndELC4uphYtWojjxcXFqpUOObPZrJo2CfqpLS9EyI2erl+/rrqnBbmRE/IiL+TGu3h02CUuLo6aN29OWVlZ4pjVaqW8vDyPLT0OnoG8yCU7O1vEyI2ckBd5ITfex+mej1u3bqkWnPn222/p1KlTFBERQbGxsZSamkrLly+ndu3aUVxcHC1ZsoSio6Pt3vVuFL4/y69//WsRDx061Kn32b59u4jffvttEfOhFj4EM2XKFJvvw+8y511znnbgwAHq1KmTtHkhUv/+P/vZz0S8ZcsWEaenp+taJq2tWrWKOnfuLHWdqc17770nYt6d/Pnnn4t43rx5Iv6///s/EfP9RPjP1qv30+Xp/v37Hiurs2SsM/wGZSKiJ554wuZ5fAGx8PBwET///PM2z7e38OHly5dF/PHHHztTVE3JmBuTyaR6zodR+PWMf48vvPCCiCsrK13+7KCgIBFPnTrVZpnWrVsn4uvXr7v8We5yuvGRn5+vWi2tegxt3LhxtHnzZpo3bx7dvn2bpkyZQiUlJdSvXz86ePAgBQcHe67U4LKXXnqJLBYL8iKZqVOnos5ICnVGXsiN93K68TFw4MBa16QICAigZcuW0bJly9wqGGjjwoULFBoaanQxoIZFixbR66+/bnQxwAbUGXkhN97L8NkuMuCLVDmy2Ne+fftEPHbsWJvntGnTRsR8qKVdu3Y2z3///fdFzLtK/VHNqdrV+FL+em79DLbxGS7du3cXcWlpqYj5gmP8OMfrHB/2rL6BnYjoypUr7hXWxxQXF6ue86ErHvMFIe0tDmnvfexdC0tKSkTMG8xoPP+AD30Qqf+OuZCQEBHzqfZ87ylnh0UGDBggYj5Cwa+XmZmZTr2nVrCrLQAAAOgKjQ8AAADQld8Mu/BtvomIUlNTRWzvDm++FwJfIMmR+1n4YmV8/QaOfy7vXrt27Vqd7+/L7O03sHr1ahHzrl++QA/vKnbnrnGw7bHHHhMxrxMc3ypc68Ws+OJJERERqtd4vXvyySdFbG/os2/fvh4unXZyc3NVz/mQBx+2jI2NFfG//vWvOt/X3rAL3zKDr6Xx6quvivjdd98VMa+f/ubu3buq5wsWLBDxqlWrRMz/3j755BMRWywWEf/73/+2+Rn2FiJbtGiRzeN8Ebq8vDyb5+gNPR8AAACgKzQ+AAAAQFcBil57uTvIarVSWFiYx9/3qaeeUj3nMyfsdTXyde9nzJhR52fwLuCPPvpIxPa6eXfv3i3iF198UcT//e9/6/wsV1ksFpenpmmVm5r40vxbt24VMe/65fiW4LzLsqCgQMR//vOfRcyH1viQjZHcyQuRfvWGd7N37NhRxAcOHBDxmDFjRFyzC7oazxPfy4fjw4+8XvKYzxioubgTH2p15DLHFzWrWVbZ64ye8vPzRcxnOW3evFnE9vb38TRvyw0fGuSL7vFZXgkJCXW+jyOzkr788ksR9+/fX8R67ObrSF7Q8wEAAAC6QuMDAAAAdOXTwy682/Uf//iH6jW+AAvvwuJdihMmTBAx36uF4wstjRs3TsR8cSXuu+++E/GcOXNEzBcu05K3dVPyrnC+/8HLL78sYntd7zyuufBPtXPnzomY79PDZ9bcvn3b2WI7TaZhF74HSM1ZFfaGEPmiehyf9cUXW+L7HTlyCbLXzXznzh0R1xyutPcz58+fFzHfR4hv7sd5W53RGl+UcefOnSLm++906tRJxHwvME/zldzwPY1qztqq1rZtWxGnpKSIODk5WcR8SHnYsGEiPnjwoEfK6SgMuwAAAIB00PgAAAAAXfn0ImP9+vUTMR9mqYnvObFx40YR2xtq4d3SfN+WxYsX2zz/5s2bIk5LSxOxXkMt3ox35aanp9uMOb5ldevWrUXctGlTEa9cuVLEP//5z0XMF4/jW3PzvyN7szd8CR8qiY+PV71mb4iEDyc6Moxib/YKryvz588XMd/vguN7vtQcIvIFvEudLxJ26dIlI4pDROoZYnyPGT6s1qFDBxFrOeziK/jwIY+5oqIiEf/tb3+zeQ5flEzvoRZnoecDAAAAdIXGBwAAAOjKp4dd7A2D1DR8+HAR27vjnQ+1HDp0SMSPPPKIiO11N/M7k/nCYuB533zzjc2Y++yzz0TMF8fiMzl4XmfOnClif9g2nO/N4i57wyKjRo2yef6RI0dEzBet8id8VldGRoaI+b4gfEhKb3zBKj7UwmcXgWc0adJExB988IGImzVrJmK+V0vNRQFlhp4PAAAA0BUaHwAAAKArnxt2GT16tIh79+5t9zw+04QPtfBtuPl+K/b2dgkM/Kn9xhd44dsl++Jd+N6G7wuzZs0aEdtbNOv69esi5sNs/oB3pdc2c2XDhg0i/vDDD0XM77jns4P4TBZ7wy5ANH78eBHz73/atGki5gtIEan3N9qzZ4+I9+/fX+fn3bt3T8R86IQvwMWHeaZOnWqzfP/5z39ELMu27d6Oz7pLTEwUMZ+hyf/N43VMdk71fKSlpVFiYiI1atSIIiMjacSIEaoVIol++ENOSUmhJk2aUMOGDSk5OVk1HQuMNWfOHORGQsiLvJAbeSE33supxkd2djalpKRQbm4uHTp0iCoqKujxxx9XLT89a9Ysev/992nXrl2UnZ1NhYWF9Oyzz3q84OCagwcPIjcSQl7khdzIC7nxXm7t7fL9999TZGQkZWdn04ABA8hisVCzZs1ox44dYv3/s2fP0sMPP0w5OTm1DoNUc3e9/W3bton4V7/6ld3z+LAL784KDg4Wsb019jneTdmzZ08R8+2My8vL63wfvWzZsoXGjh1LRPrnRisNGzYUMV80jHft8xkrfC8Yjg+bLVq0SMSvvfaaR8pZG3fyQuTZ3Ozdu1fEhYWFqtf4nkV8y3tn8e+6rKxMxHy4kg/fGEnvOsMX6OIzXPi267VxZLt1ji9Yxn82Jiamzp+9ePGiiAcOHGjzPbXki9czPuz29ttvi9hsNot4+vTpIn7nnXd0KZczNN/bxWKxENFP/0ifPHmSKioqVOPrHTp0oNjYWMrJybH5HmVlZWS1WlUP0A6/QCA38nAmL0TIjZ5QZ+SF3HgvlxsfVVVVlJqaSn379qWEhAQi+mH5V5PJpFoTg4goKipKtTQsl5aWRmFhYeLhSGsbXIfcyMmZvBAhN3pCnZEXcuO9XJ7tkpKSQqdPn1at8++KhQsX0uzZs8Vzq9Xq1h/Fe++9J+Lahl34wmLOdlPyfSx++9vfiljWoRZXeTo39tSrp/4zHDNmjIj5MBj/rrnIyEgRd+zY0eVyrFixQsR6DLW4Q8vc8DvstcLrWX5+vohlGWpxlSfycvbsWRHz61Tjxo1FzHuXiYiWLl0qYn4940PHfGEqjneP8xs2eTn4MAqfTbNjxw4R29t/RxZ6Xc9cwfePWr9+vYiDgoJEzPcdk3GoxVkuNT6mT59O+/fvp2PHjlGrVq3E8ebNm1N5eTmVlJSoWqTFxcWq6Xuc2WxWjWWBtkpKSh642CA3xnMmL0TIjZ5QZ+SF3Hgvp4ZdFEWh6dOn0549e+jw4cMUFxener1Hjx5Uv359ysrKEsfOnTtHly5doqSkJM+UGNzC1zRBbuSBvMgLuZEXcuO9nOr5SElJoR07dtC+ffuoUaNGYmwtLCyMQkJCKCwsjCZOnEizZ8+miIgICg0NpRkzZlBSUpLDd+2DthYtWkStWrVCbiSDvMgLuZEXcuO9nGp8VI8z8TuMiYg2bdokpgetWbOGAgMDKTk5mcrKymjw4MGq6UJa4ysq8vsuTCaT0+/FV7nkqzny6bxff/210+9rpMGDBxuWG3tqbkjVuXNnEc+aNUvEruSwGl/Fka8GuXPnThGnpaW5/P7ukjEv8AMjc8OnJPPVK/nfra3n1Vq2bCni1q1b2zyHX+dwPdNPdHS0iJcvXy5ifp8H3wSTr3DrC5xqfDhyM2ZwcDBlZGSodmMEefzhD39QNaRADsiLvJAbeSE33gsbywEAAICufG5jOb6R0oQJE0T8u9/9TnVe165dRTxnzhyb78W7Mt1ZzRFqV1FRoXq+YMECEfNpfZMmTRIxXwWSTwnkew3x1U7T09NF/MUXX7hZYnAX39yPr5IJnnX16lWbMRiDzwJ9/fXXRdy/f38RX7lyRcTVK4UTEVVWVmpbOJ2h5wMAAAB0hcYHAAAA6MqtjeW0IOtmP77CkQ1/7EFutONOXoiQGy2hzsjL23LDN20cNmyYzXPGjRsn4r/85S9aF0kTmm8sBwAAAOAsND4AAABAVz432wUAAEBGemza6C3Q8wEAAAC6QuMDAAAAdIXGBwAAAOgKjQ8AAADQFRofAAAAoCs0PgAAAEBXaHwAAACArqRrfEi22rvPcef7RW604+53i9xoB3VGXsiNnBz5bqVrfJSWlhpdBJ/mzveL3GjH3e8WudEO6oy8kBs5OfLdSrexXFVVFRUWFpKiKBQbG0uXL192a8Mtb2K1WikmJkaT31lRFCotLaXo6GgKDHStzVlVVUXnzp2jjh07+lVeiLTLjSfyQuS/ufGGOoPrmby5QZ0xLi/SLa8eGBhIrVq1IqvVSkREoaGhfvNHUU2r39ndHRwDAwOpZcuWROSfeSHS5vf2xM6a/p4bmesMrmfy5gZ1xri8SDfsAgAAAL4NjQ8AAADQlbSND7PZTK+88gqZzWaji6Ibb/idvaGMWvCG39sbyuhp3vI7e0s5PckbfmdvKKOnyfI7S3fDKQAAAPg2aXs+AAAAwDeh8QEAAAC6QuMDAAAAdIXGBwAAAOgKjQ8AAADQlZSNj4yMDGrTpg0FBwdTr1696MSJE0YXyWPS0tIoMTGRGjVqRJGRkTRixAg6d+6c6px79+5RSkoKNWnShBo2bEjJyclUXFxsUInVkBvkRm/Ii7yQG3lJnxtFMpmZmYrJZFI2btyonDlzRpk8ebISHh6uFBcXG100jxg8eLCyadMm5fTp08qpU6eUoUOHKrGxscqtW7fEOdOmTVNiYmKUrKwsJT8/X+ndu7fSp08fA0v9A+QGuTEC8iIv5EZesudGusZHz549lZSUFPG8srJSiY6OVtLS0gwslXauX7+uEJGSnZ2tKIqilJSUKPXr11d27dolzvnqq68UIlJycnKMKqaiKMgNciMH5EVeyI28ZMuNVMMu5eXldPLkSRo0aJA4FhgYSIMGDaKcnBwDS6Ydi8VCREQRERFERHTy5EmqqKhQfQcdOnSg2NhYQ78D5Aa5kQXyIi/kRl6y5UaqxseNGzeosrKSoqKiVMejoqKoqKjIoFJpp6qqilJTU6lv376UkJBARERFRUVkMpkoPDxcda7R3wFyg9zIAHmRF3IjLxlzU0/zTwC7UlJS6PTp03T8+HGjiwI1IDdyQl7khdzIS8bcSNXz0bRpUwoKCnrgbtvi4mJq3ry5QaXSxvTp02n//v105MgRatWqlTjevHlzKi8vp5KSEtX5Rn8HyA1yYzTkRV7IjbxkzY1UjQ+TyUQ9evSgrKwscayqqoqysrIoKSnJwJJ5jqIoNH36dNqzZw8dPnyY4uLiVK/36NGD6tevr/oOzp07R5cuXTL0O0BukBujIC/yQm7kJX1uNL+l1UmZmZmK2WxWNm/erBQUFChTpkxRwsPDlaKiIqOL5hEvvPCCEhYWphw9elS5du2aeNy5c0ecM23aNCU2NlY5fPiwkp+fryQlJSlJSUkGlvoHyA1yYwTkRV7Ijbxkz410jQ9FUZT09HQlNjZWMZlMSs+ePZXc3Fyji+QxRGTzsWnTJnHO3bt3lRdffFFp3Lix8tBDDynPPPOMcu3aNeMKzSA3yI3ekBd5ITfykj03AT8WEgAAAEAXUt3zAQAAAL4PjQ8AAADQFRofAAAAoCs0PgAAAEBXaHwAAACArtD4AAAAAF2h8QEAAAC6QuMDAAAAdIXGBwAAAOgKjQ8AAADQFRofAAAAoKv/B5Fobzxxk2G6AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ------------------------------------\n",
        "for num in range(0,5):\n",
        "    one_data = next(iter(train_loader))\n",
        "    plt.subplot(1,5,num+1)\n",
        "    plt.imshow(one_data[0][0][0,:,:], cmap=\"grey\")\n",
        "    plt.title(one_data[1][0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo6-KX8UyZCy"
      },
      "source": [
        "## Network setting\n",
        "\n",
        "While image classification, is usually performed using Convolutional Neural Networks, we will use here a Multi-Layer-Perceptron (also named ```fully-connected```).\n",
        "For this, we will flatten (i.e. convert the iamge 2D matrix to a 1D vector) the input images.\n",
        "\n",
        "- The input images are of size 28*28 and are converted to vectors of size ```n_in```=784.\n",
        "\n",
        "- The two hidden layers have````n_h1````=500 and ```n_h2```=256 unit/neurons each and ```Relu```activations.\n",
        "\n",
        "- The output are the ```n_out```=10 classes (the 10 digits to be recognized).\n",
        "\n",
        "Since we deal with a muti-class problem (10 classes), the output activations is therefore a ```softmax```.\n",
        "\n",
        "Note that since the ```softmax```output will be given to a ```cross-entropy```, the ```log```et the latter can be directly added to the ```softmax```. This leads to the pytorch function ```F.log_softmax```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "H0uIIQb9yZCy"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        n_in = 28*28\n",
        "        n_h1 = 500\n",
        "        n_h2 = 256\n",
        "        n_out = 10\n",
        "        super(Net, self).__init__()\n",
        "        # --- START CODE HERE  (02)\n",
        "\n",
        "        self.input = nn.Linear(n_in, n_h1, device=\"cuda\")\n",
        "        self.activation = nn.ReLU()\n",
        "        self.hidden1 = nn.Linear(n_h1, n_h2, device=\"cuda\")\n",
        "        self.output = nn.Linear(n_h2, n_out, device=\"cuda\")\n",
        "\n",
        "        # --- END CODE HERE\n",
        "    def forward(self, x):\n",
        "        # --- START CODE HERE  (03)\n",
        "        \n",
        "        x = x.to(\"cuda\")\n",
        "        x = x.flatten(start_dim = 1)\n",
        "        \n",
        "        x = self.input(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.output(x)\n",
        "        \n",
        "        # --- END CODE HERE\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7qcgKFwyZCz"
      },
      "source": [
        "## Defining the train and test functions\n",
        "\n",
        "In the present lab, we distinguish between train and test data.\n",
        "We therefore write\n",
        "- a train function (to learn the network parameters on the train data),\n",
        "- a test function (to evaluate the network's performances on the test data).\n",
        "\n",
        "The ```train``` and ```test```function will perform the computation for a given ```epoch```.\n",
        "The functions therefore only need to ```loop over``` the mini-batches.\n",
        "This is now easy since we defined our ```Dataloader```for train and test.\n",
        "\n",
        "For the train function, for each mini-batch, we  \n",
        "- compute the forward pass by passing the data to the model: haty = model(x)\n",
        "- compute the the loss (the criterion)\n",
        "- putting at zero the gradients of all the parameters of the network (this is important since, by default, pytorch accumulate the gradients over time)\n",
        "- computing the backpropagation (using as before .backward())\n",
        "- performing one step of optimization (using .step())\n",
        "\n",
        "For the train function, we set the model to train mode (```model.train()```). This allows to set some specific behaviours for training (such as activate the dropout).\n",
        "\n",
        "For the test function, we set the model to train mode (```model.eval()```). This allows to set some specific behaviours for training (such as removing the dropout).\n",
        "\n",
        "\n",
        "Since we are dealing with a multi-class problem (10 classes), we will minimize as loss the ```cross-entropy``` (named negative-log-likelihood ```nll_loss```in pytorch).\n",
        "Since ```nll_loss```does not contains the ```log``` we have added it directly to the softmax by using the ```log_softmax```as output activation of our network.\n",
        "\n",
        "Note that we could use the loss from the nn package (```torch.nn.NLLLoss```) it is then a class which needs to be first instanciated.\n",
        "We can also use the same but as a function (```F.nll_loss()```) which can be used directly without instanciating the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3dGuPt96yZCz"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # --- START CODE HERE  (04)\n",
        "\n",
        "        data , target = data.to(\"cuda\"), target.to(\"cuda\")\n",
        "\n",
        "        haty = model(data)\n",
        "        loss = F.nll_loss(haty, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # --- END CODE HERE\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI8QtzV2yZC0"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    \n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for data, target in test_loader:\n",
        "            # --- START CODE HERE  (05)\n",
        "            \n",
        "            data, target = data.to(\"cuda\"), target.to(\"cuda\")\n",
        "\n",
        "            predict = model(data)\n",
        "\n",
        "            loss = F.nll_loss(predict, target)\n",
        "            test_loss += loss\n",
        "\n",
        "            predict = predict.argmax(dim = 1)\n",
        "\n",
        "            correct += predict.eq(target).sum().item()\n",
        "            # --- STOP CODE HERE\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    correct /= len(test_loader.dataset)\n",
        "    \n",
        "    print('\\nTest set: Average loss: {}, Accuracy: {}\\n'.format(test_loss, correct))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MXQIbcJyZC0"
      },
      "source": [
        "## Looping over epochs\n",
        "\n",
        "\n",
        "We finally define, how we will optimize our parameters by defining an optimizer.\n",
        "We will use here a very simple one: the SGD (```optim.SGD```) with a learning rate of 0.01.\n",
        "\n",
        "At each iteration, we then call the train and the test function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ofYcX05ZyZC0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1, loss 2.3239283561706543\n",
            "epoch 1, loss 1.3169353008270264\n",
            "epoch 1, loss 0.30876511335372925\n",
            "epoch 1, loss 0.20389896631240845\n",
            "epoch 1, loss 0.1377960741519928\n",
            "epoch 1, loss 0.5478036403656006\n",
            "epoch 1, loss 0.12314974516630173\n",
            "epoch 1, loss 0.22210228443145752\n",
            "epoch 1, loss 0.012384897097945213\n",
            "epoch 1, loss 0.041621703654527664\n",
            "epoch 1, loss 0.6564128398895264\n",
            "epoch 1, loss 0.702899694442749\n",
            "epoch 1, loss 0.23938032984733582\n",
            "epoch 1, loss 0.26413533091545105\n",
            "epoch 1, loss 0.13203923404216766\n",
            "epoch 1, loss 0.03061678633093834\n",
            "epoch 1, loss 0.04489133879542351\n",
            "epoch 1, loss 0.2853321135044098\n",
            "epoch 1, loss 0.20545589923858643\n",
            "epoch 1, loss 0.11741313338279724\n",
            "epoch 1, loss 0.7874047160148621\n",
            "epoch 1, loss 0.06174908205866814\n",
            "epoch 1, loss 0.035504139959812164\n",
            "epoch 1, loss 0.20293238759040833\n",
            "epoch 1, loss 0.30054670572280884\n",
            "epoch 1, loss 0.0759253054857254\n",
            "epoch 1, loss 0.18300634622573853\n",
            "epoch 1, loss 0.007961612194776535\n",
            "epoch 1, loss 0.02294863760471344\n",
            "epoch 1, loss 0.14176298677921295\n",
            "epoch 1, loss 0.21932408213615417\n",
            "epoch 1, loss 0.0774700939655304\n",
            "epoch 1, loss 0.07569363713264465\n",
            "epoch 1, loss 0.1671164184808731\n",
            "epoch 1, loss 0.008482340723276138\n",
            "epoch 1, loss 0.011608668603003025\n",
            "epoch 1, loss 0.04260663688182831\n",
            "epoch 1, loss 0.04090843349695206\n",
            "epoch 2, loss 0.10477599501609802\n",
            "epoch 2, loss 0.0024670669808983803\n",
            "epoch 2, loss 0.020722752436995506\n",
            "epoch 2, loss 0.3288687765598297\n",
            "epoch 2, loss 0.06225152686238289\n",
            "epoch 2, loss 0.3700030446052551\n",
            "epoch 2, loss 0.004310500808060169\n",
            "epoch 2, loss 0.027850624173879623\n",
            "epoch 2, loss 0.3201327919960022\n",
            "epoch 2, loss 0.00802187342196703\n",
            "epoch 2, loss 0.037364184856414795\n",
            "epoch 2, loss 0.006957433186471462\n",
            "epoch 2, loss 0.05175838619470596\n",
            "epoch 2, loss 0.02749880775809288\n",
            "epoch 2, loss 0.03738442808389664\n",
            "epoch 2, loss 0.04772742837667465\n",
            "epoch 2, loss 0.5988794565200806\n",
            "epoch 2, loss 0.028146451339125633\n",
            "epoch 2, loss 0.017997322604060173\n",
            "epoch 2, loss 0.29481223225593567\n",
            "epoch 2, loss 0.266547292470932\n",
            "epoch 2, loss 0.009904536418616772\n",
            "epoch 2, loss 0.03703185170888901\n",
            "epoch 2, loss 0.06706450879573822\n",
            "epoch 2, loss 0.010542160831391811\n",
            "epoch 2, loss 0.13039961457252502\n",
            "epoch 2, loss 0.32274022698402405\n",
            "epoch 2, loss 0.07029397040605545\n",
            "epoch 2, loss 0.25290408730506897\n",
            "epoch 2, loss 0.29084688425064087\n",
            "epoch 2, loss 0.25563138723373413\n",
            "epoch 2, loss 0.0014539121184498072\n",
            "epoch 2, loss 0.00815396849066019\n",
            "epoch 2, loss 0.006311286240816116\n",
            "epoch 2, loss 0.016590027138590813\n",
            "epoch 2, loss 0.05412360280752182\n",
            "epoch 2, loss 0.17700977623462677\n",
            "epoch 2, loss 0.03365008533000946\n",
            "epoch 3, loss 0.044899579137563705\n",
            "epoch 3, loss 0.006091575138270855\n",
            "epoch 3, loss 0.22394287586212158\n",
            "epoch 3, loss 0.047391824424266815\n",
            "epoch 3, loss 0.008942188695073128\n",
            "epoch 3, loss 0.17400968074798584\n",
            "epoch 3, loss 0.014433179050683975\n",
            "epoch 3, loss 0.015186568722128868\n",
            "epoch 3, loss 0.02800467237830162\n",
            "epoch 3, loss 0.017058735713362694\n",
            "epoch 3, loss 0.017712993547320366\n",
            "epoch 3, loss 0.028180042281746864\n",
            "epoch 3, loss 0.01327736210078001\n",
            "epoch 3, loss 0.05329252779483795\n",
            "epoch 3, loss 0.0072912415489554405\n",
            "epoch 3, loss 0.08534253388643265\n",
            "epoch 3, loss 0.01942511461675167\n",
            "epoch 3, loss 0.021104510873556137\n",
            "epoch 3, loss 0.028255075216293335\n",
            "epoch 3, loss 0.008892144076526165\n",
            "epoch 3, loss 0.0014990025665611029\n",
            "epoch 3, loss 0.01428380236029625\n",
            "epoch 3, loss 0.143493190407753\n",
            "epoch 3, loss 0.06638002395629883\n",
            "epoch 3, loss 0.04889390245079994\n",
            "epoch 3, loss 0.23072656989097595\n",
            "epoch 3, loss 0.13577154278755188\n",
            "epoch 3, loss 0.007868616841733456\n",
            "epoch 3, loss 0.13298310339450836\n",
            "epoch 3, loss 0.0022145549301058054\n",
            "epoch 3, loss 0.08478613197803497\n",
            "epoch 3, loss 0.06597946584224701\n",
            "epoch 3, loss 0.05085942521691322\n",
            "epoch 3, loss 0.03180219233036041\n",
            "epoch 3, loss 0.008221269585192204\n",
            "epoch 3, loss 0.0003073878469876945\n",
            "epoch 3, loss 0.0011814202880486846\n",
            "epoch 3, loss 0.16506309807300568\n",
            "epoch 4, loss 0.07780981808900833\n",
            "epoch 4, loss 0.00025761345750652254\n",
            "epoch 4, loss 0.001999877393245697\n",
            "epoch 4, loss 0.06886260956525803\n",
            "epoch 4, loss 0.032538022845983505\n",
            "epoch 4, loss 0.039399027824401855\n",
            "epoch 4, loss 0.009207656607031822\n",
            "epoch 4, loss 0.07428011298179626\n",
            "epoch 4, loss 0.37510350346565247\n",
            "epoch 4, loss 0.026724739000201225\n",
            "epoch 4, loss 0.08132324367761612\n",
            "epoch 4, loss 0.07699282467365265\n",
            "epoch 4, loss 0.0025510601699352264\n",
            "epoch 4, loss 0.007336942944675684\n",
            "epoch 4, loss 0.06920259445905685\n",
            "epoch 4, loss 0.054815009236335754\n",
            "epoch 4, loss 0.0001759915758157149\n",
            "epoch 4, loss 0.007719138171523809\n",
            "epoch 4, loss 0.05319761857390404\n",
            "epoch 4, loss 0.00680456543341279\n",
            "epoch 4, loss 0.032842062413692474\n",
            "epoch 4, loss 0.0020484570413827896\n",
            "epoch 4, loss 0.07507456839084625\n",
            "epoch 4, loss 0.05712493136525154\n",
            "epoch 4, loss 0.0009847789769992232\n",
            "epoch 4, loss 0.036454834043979645\n",
            "epoch 4, loss 0.0026553443167358637\n",
            "epoch 4, loss 0.007591001223772764\n",
            "epoch 4, loss 0.03239103779196739\n",
            "epoch 4, loss 0.20089854300022125\n",
            "epoch 4, loss 0.00043081058538518846\n",
            "epoch 4, loss 0.02251501753926277\n",
            "epoch 4, loss 0.0038142951671034098\n",
            "epoch 4, loss 0.003577430034056306\n",
            "epoch 4, loss 0.004602950531989336\n",
            "epoch 4, loss 0.03961937129497528\n",
            "epoch 4, loss 0.0003004353493452072\n",
            "epoch 4, loss 0.005008340813219547\n",
            "epoch 5, loss 0.0041745551861822605\n",
            "epoch 5, loss 0.002167914528399706\n",
            "epoch 5, loss 0.06296668201684952\n",
            "epoch 5, loss 0.0047475374303758144\n",
            "epoch 5, loss 0.25180569291114807\n",
            "epoch 5, loss 0.021212980151176453\n",
            "epoch 5, loss 0.07188321650028229\n",
            "epoch 5, loss 0.39164823293685913\n",
            "epoch 5, loss 0.01268120389431715\n",
            "epoch 5, loss 0.0020729554817080498\n",
            "epoch 5, loss 0.0007151065510697663\n",
            "epoch 5, loss 0.0009257262572646141\n",
            "epoch 5, loss 0.000833165249787271\n",
            "epoch 5, loss 0.1010209247469902\n",
            "epoch 5, loss 0.0068142833188176155\n",
            "epoch 5, loss 0.013116064481437206\n",
            "epoch 5, loss 0.00027178716845810413\n",
            "epoch 5, loss 0.03190961107611656\n",
            "epoch 5, loss 0.21308140456676483\n",
            "epoch 5, loss 0.0005718670436181128\n",
            "epoch 5, loss 0.004890558775514364\n",
            "epoch 5, loss 0.0037310721818357706\n",
            "epoch 5, loss 0.006057534366846085\n",
            "epoch 5, loss 0.00686010904610157\n",
            "epoch 5, loss 0.26912760734558105\n",
            "epoch 5, loss 0.003561496501788497\n",
            "epoch 5, loss 0.0008442375110462308\n",
            "epoch 5, loss 0.0065568010322749615\n",
            "epoch 5, loss 0.0683293268084526\n",
            "epoch 5, loss 0.012777564115822315\n",
            "epoch 5, loss 0.00010845089855138212\n",
            "epoch 5, loss 0.00212684809230268\n",
            "epoch 5, loss 0.1414785534143448\n",
            "epoch 5, loss 0.0008803271339274943\n",
            "epoch 5, loss 0.09649817645549774\n",
            "epoch 5, loss 0.00025232284679077566\n",
            "epoch 5, loss 0.0009389602346345782\n",
            "epoch 5, loss 0.0005983133451081812\n",
            "epoch 6, loss 0.012725668027997017\n",
            "epoch 6, loss 0.07059702277183533\n",
            "epoch 6, loss 0.010399422608315945\n",
            "epoch 6, loss 0.0032565922010689974\n",
            "epoch 6, loss 0.00024601680343039334\n",
            "epoch 6, loss 0.001838894560933113\n",
            "epoch 6, loss 0.03961081802845001\n",
            "epoch 6, loss 0.0036260865163058043\n",
            "epoch 6, loss 0.02742641605436802\n",
            "epoch 6, loss 0.0020283241756260395\n",
            "epoch 6, loss 0.001184143009595573\n",
            "epoch 6, loss 0.0003630380961112678\n",
            "epoch 6, loss 0.001059658476151526\n",
            "epoch 6, loss 0.0016926996177062392\n",
            "epoch 6, loss 0.030297204852104187\n",
            "epoch 6, loss 0.005201379302889109\n",
            "epoch 6, loss 0.09408504515886307\n",
            "epoch 6, loss 0.0006734653143212199\n",
            "epoch 6, loss 0.0008550253696739674\n",
            "epoch 6, loss 0.00037231954047456384\n",
            "epoch 6, loss 0.006520536262542009\n",
            "epoch 6, loss 0.0001971498568309471\n",
            "epoch 6, loss 0.0010396293364465237\n",
            "epoch 6, loss 0.005790710914880037\n",
            "epoch 6, loss 0.0021821416448801756\n",
            "epoch 6, loss 0.0004054098390042782\n",
            "epoch 6, loss 0.00034410462831147015\n",
            "epoch 6, loss 0.014634394086897373\n",
            "epoch 6, loss 0.04398295655846596\n",
            "epoch 6, loss 0.0012713194591924548\n",
            "epoch 6, loss 0.013839419931173325\n",
            "epoch 6, loss 0.002887961221858859\n",
            "epoch 6, loss 0.0018965282943099737\n",
            "epoch 6, loss 0.02068031020462513\n",
            "epoch 6, loss 0.029253289103507996\n",
            "epoch 6, loss 0.007622396573424339\n",
            "epoch 6, loss 0.000983114936389029\n",
            "epoch 6, loss 0.005081058945506811\n",
            "epoch 7, loss 0.013973761349916458\n",
            "epoch 7, loss 0.001348684192635119\n",
            "epoch 7, loss 0.0013383077457547188\n",
            "epoch 7, loss 0.008709353394806385\n",
            "epoch 7, loss 0.0002446680737193674\n",
            "epoch 7, loss 0.005098110064864159\n",
            "epoch 7, loss 0.0031081773340702057\n",
            "epoch 7, loss 0.024019839242100716\n",
            "epoch 7, loss 0.001243920880369842\n",
            "epoch 7, loss 0.0005417082575149834\n",
            "epoch 7, loss 0.0012167957611382008\n",
            "epoch 7, loss 0.004258082713931799\n",
            "epoch 7, loss 0.0019925374072045088\n",
            "epoch 7, loss 0.25887182354927063\n",
            "epoch 7, loss 0.0007257777033373713\n",
            "epoch 7, loss 0.04663481190800667\n",
            "epoch 7, loss 7.052028377074748e-05\n",
            "epoch 7, loss 0.03545926511287689\n",
            "epoch 7, loss 0.000259960419498384\n",
            "epoch 7, loss 0.0001424003712600097\n",
            "epoch 7, loss 0.016223279759287834\n",
            "epoch 7, loss 0.0018081089947372675\n",
            "epoch 7, loss 0.00022571675071958452\n",
            "epoch 7, loss 0.000274543184787035\n",
            "epoch 7, loss 2.1084293621242978e-05\n",
            "epoch 7, loss 0.22185568511486053\n",
            "epoch 7, loss 0.0016579140210524201\n",
            "epoch 7, loss 0.0005536242970265448\n",
            "epoch 7, loss 0.07406982034444809\n",
            "epoch 7, loss 0.0033728708513081074\n",
            "epoch 7, loss 0.00122215470764786\n",
            "epoch 7, loss 0.0012269978178665042\n",
            "epoch 7, loss 0.03119192086160183\n",
            "epoch 7, loss 0.08463463932275772\n",
            "epoch 7, loss 8.366415568161756e-05\n",
            "epoch 7, loss 0.00014318818284664303\n",
            "epoch 7, loss 0.0002578411076683551\n",
            "epoch 7, loss 0.0002562427252996713\n",
            "epoch 8, loss 0.008783419616520405\n",
            "epoch 8, loss 0.0006228263955563307\n",
            "epoch 8, loss 0.0003890810767188668\n",
            "epoch 8, loss 0.002267957665026188\n",
            "epoch 8, loss 0.0014079505344852805\n",
            "epoch 8, loss 7.576911593787372e-05\n",
            "epoch 8, loss 0.0018814140930771828\n",
            "epoch 8, loss 0.0005719558103010058\n",
            "epoch 8, loss 0.0006709414883516729\n",
            "epoch 8, loss 0.00048730315756984055\n",
            "epoch 8, loss 0.01992552913725376\n",
            "epoch 8, loss 0.0003439287538640201\n",
            "epoch 8, loss 0.0007460930501110852\n",
            "epoch 8, loss 0.0006909916992299259\n",
            "epoch 8, loss 0.052195385098457336\n",
            "epoch 8, loss 2.6567442546365783e-05\n",
            "epoch 8, loss 0.09667977690696716\n",
            "epoch 8, loss 0.0031445124186575413\n",
            "epoch 8, loss 0.0014027428114786744\n",
            "epoch 8, loss 0.002439429983496666\n",
            "epoch 8, loss 0.025994649156928062\n",
            "epoch 8, loss 0.0005282282363623381\n",
            "epoch 8, loss 5.846227577421814e-05\n",
            "epoch 8, loss 0.000795319676399231\n",
            "epoch 8, loss 0.0031385659240186214\n",
            "epoch 8, loss 0.0032709999941289425\n",
            "epoch 8, loss 0.15193577110767365\n",
            "epoch 8, loss 0.035132020711898804\n",
            "epoch 8, loss 0.0014176948461681604\n",
            "epoch 8, loss 0.1578882485628128\n",
            "epoch 8, loss 0.0014123953878879547\n",
            "epoch 8, loss 0.000633091724012047\n",
            "epoch 8, loss 0.0002887988812290132\n",
            "epoch 8, loss 0.0071146502159535885\n",
            "epoch 8, loss 0.008086332120001316\n",
            "epoch 8, loss 0.0007638501119799912\n",
            "epoch 8, loss 0.0005587449413724244\n",
            "epoch 8, loss 3.196264060534304e-06\n",
            "epoch 9, loss 4.913569864584133e-05\n",
            "epoch 9, loss 0.0009943533223122358\n",
            "epoch 9, loss 0.0006264043040573597\n",
            "epoch 9, loss 0.0034949735272675753\n",
            "epoch 9, loss 0.016714634373784065\n",
            "epoch 9, loss 0.04707713797688484\n",
            "epoch 9, loss 0.006563689559698105\n",
            "epoch 9, loss 0.00014995073433965445\n",
            "epoch 9, loss 0.0003389226330909878\n",
            "epoch 9, loss 0.03284982219338417\n",
            "epoch 9, loss 0.0003423553134780377\n",
            "epoch 9, loss 0.000931460817810148\n",
            "epoch 9, loss 0.0006798431277275085\n",
            "epoch 9, loss 0.0004792989930137992\n",
            "epoch 9, loss 0.005645857658237219\n",
            "epoch 9, loss 0.000514968647621572\n",
            "epoch 9, loss 0.0024221870116889477\n",
            "epoch 9, loss 0.06882432848215103\n",
            "epoch 9, loss 0.0001906361139845103\n",
            "epoch 9, loss 0.0007031538407318294\n",
            "epoch 9, loss 0.00027390901232138276\n",
            "epoch 9, loss 8.128439731081016e-06\n",
            "epoch 9, loss 0.007454403210431337\n",
            "epoch 9, loss 0.0013008011737838387\n",
            "epoch 9, loss 1.9176526620867662e-05\n",
            "epoch 9, loss 0.002815449610352516\n",
            "epoch 9, loss 0.00032058381475508213\n",
            "epoch 9, loss 0.00021150420070625842\n",
            "epoch 9, loss 7.737042324151844e-05\n",
            "epoch 9, loss 0.0039560431614518166\n",
            "epoch 9, loss 0.00012544810306280851\n",
            "epoch 9, loss 0.040753450244665146\n",
            "epoch 9, loss 0.004155673552304506\n",
            "epoch 9, loss 4.084482498001307e-05\n",
            "epoch 9, loss 5.142093141330406e-05\n",
            "epoch 9, loss 0.0010776720009744167\n",
            "epoch 9, loss 5.735900049330667e-05\n",
            "epoch 9, loss 1.57721897267038e-05\n",
            "epoch 10, loss 0.021802378818392754\n",
            "epoch 10, loss 0.0017431864980608225\n",
            "epoch 10, loss 0.004955731797963381\n",
            "epoch 10, loss 0.00017988728359341621\n",
            "epoch 10, loss 0.0027587981894612312\n",
            "epoch 10, loss 0.0007476721075363457\n",
            "epoch 10, loss 0.005579951219260693\n",
            "epoch 10, loss 0.0030516551341861486\n",
            "epoch 10, loss 0.0025780824944376945\n",
            "epoch 10, loss 0.00023792119463905692\n",
            "epoch 10, loss 0.005893986206501722\n",
            "epoch 10, loss 2.3720775061519817e-05\n",
            "epoch 10, loss 0.06830045580863953\n",
            "epoch 10, loss 0.0005788515554741025\n",
            "epoch 10, loss 0.0002221526810899377\n",
            "epoch 10, loss 0.0006294132908806205\n",
            "epoch 10, loss 0.06805478781461716\n",
            "epoch 10, loss 0.0006052113021723926\n",
            "epoch 10, loss 0.00035511201713234186\n",
            "epoch 10, loss 0.013168487697839737\n",
            "epoch 10, loss 0.001692996476776898\n",
            "epoch 10, loss 0.00011702453775797039\n",
            "epoch 10, loss 0.00015610147966071963\n",
            "epoch 10, loss 0.0007041559438221157\n",
            "epoch 10, loss 0.000393061462091282\n",
            "epoch 10, loss 2.6595802410156466e-05\n",
            "epoch 10, loss 0.008773889392614365\n",
            "epoch 10, loss 0.00013177802611608058\n",
            "epoch 10, loss 0.013417094945907593\n",
            "epoch 10, loss 0.0012458895798772573\n",
            "epoch 10, loss 0.00016691327618900687\n",
            "epoch 10, loss 0.00016424639034084976\n",
            "epoch 10, loss 0.0014615969266742468\n",
            "epoch 10, loss 0.00020239768491592258\n",
            "epoch 10, loss 3.309847670607269e-05\n",
            "epoch 10, loss 0.0018013514345511794\n",
            "epoch 10, loss 0.0013968461425974965\n",
            "epoch 10, loss 0.010934947058558464\n"
          ]
        }
      ],
      "source": [
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "nb_epoch = 10\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "        # --- START CODE HERE  (06)\n",
        "        train(model, train_loader, optimizer, epoch)\n",
        "        # --- END CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.003857350442558527, Accuracy: 0.9847\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXOlN4Bj4Sk0"
      },
      "source": [
        "# Peer-grading\n",
        "\n",
        "To great the Lab, you will apply the following:\n",
        "- When the code of a part (each part is numbered as \"START CODE (01)\" ... \"START CODE (06)\") is correct you get 1.5 point.\n",
        "- You will give one extra point is all the code is correct and run smoothly\n",
        "- So the total for Lab is 10 points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmga0J_oyZC1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
