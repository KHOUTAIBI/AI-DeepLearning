{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSIA202a - Third Practice Session\n",
    "\n",
    "The goal of this third partical work is to use an AR(p) process to model a speech signal, and show that this approch is surprisingly powerful despite its simplicity.\n",
    "\n",
    "The proposed synthesis algorithm divides into several steps.\n",
    "1. Filter the audio signal to emphasize the highest frequencies (in order to flatten the spectrum and reduce precision issues with FFT computations).\n",
    "2. Split the signal into several overlapping frames and iterate over them.\n",
    "    1. Detect if the frame corresponds to noise or to a voiced signal, and estimate the fundamental frequency in the later case.\n",
    "    2. Estimate the AR(p) coefficients using Yule-Walker equations.\n",
    "    3. Re-synthetise the frame using the AR(p) coefficients, starting from a white noise in the case of a noisy frame, or a Dirac comb in the case of a voiced frame.\n",
    "3. Overlap-add the synthesized frames with a Hanning window.\n",
    "4. Filter the synthesised signal to de-emphasize the highest frequencies.\n",
    "\n",
    "We propose to implement this algorithm in two parts. First, we will derive the Yule-Walker equations, and test them on synthetic data. Then, we will code the synthesis algorithm and apply it on a real world speech signal.\n",
    "\n",
    "\n",
    "# 1. Yule-Walker equations\n",
    "\n",
    "Let us consider a causal, zero-mean, AR(p) process defined by the following recurrent equation: \n",
    "$$\n",
    "X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\ldots + \\phi_p X_{t-p} + Z_{t}\n",
    "$$\n",
    "where $\\{Z_t , t \\in \\mathbb{Z}\\}$ is a weak white noise with variance $\\sigma^2$.\n",
    "\n",
    "1. Show that, $\\forall h \\geq 1, \\mathbb{E}[X_{t-h}Z_t]=0$.\n",
    "2. Deduce a recurrent relation between $\\gamma(h)$ and $\\gamma(h-1),\\gamma(h-2),\\ldots,\\gamma(h-p)$, for $h \\geq 1$.\n",
    "3. We consider separately the case $h=0$: find a new relationship between $\\gamma(0)$ and $\\gamma(-1),\\gamma(-2),\\ldots,\\gamma(-p)$.\n",
    "4. Put these relationships in matrix form:\n",
    "\\begin{equation}\n",
    "\\Gamma_{p+1}[1 \\; -\\phi_1 \\; \\ldots \\; -\\phi_p]^T = [\\sigma_2 \\; 0 \\; \\ldots \\; 0]^T \n",
    "\\end{equation}\n",
    "where $\\Gamma_{p+1}$ is a suitable Toeplitz matrix that you have to determine.\n",
    "\n",
    "# 2. Evaluation on synthetic data\n",
    "\n",
    "We will now evaluate equation $(1)$ on a synthetically generated AR(p) process.\n",
    "\n",
    "1. Generate n=1000 samples of an AR(4) process.\n",
    "2. Estimate $\\Gamma_{p+1}$ using these samples.\n",
    "3. Use equation $(1)$ to estimate $\\sigma^2$ and the coefficients $\\phi_1,\\phi_2,\\ldots,\\phi_p$. Compute the relative error between the estimated and the true coefficients.\n",
    "4. Estimate power spectral distribution of the synthesized signal. Compare with the theoretical distribution obtained from the poles of $1/\\Phi(z^{-1})$.\n",
    "\n",
    "# 3. Speech modeling\n",
    "\n",
    "We will now implement the synthesis algorithm and evaluate it on a speech signal. \n",
    "\n",
    "A code template is provided below. We will use the file audio.wav to test the algorithm. \n",
    "\n",
    "\n",
    "LIsten to the original and synthesized signal by using any audio player. Try to modify the pitch of the synthesised signal and listen to the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import scipy\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparameters\n",
    "\n",
    "sampling_rate = 8000 # (Hz)\n",
    "frame_duration = 0.04 # duration of the analysis frames (seconds)\n",
    "overlap_ratio = 0.5 # overlap ratio between two consecutive analysis frames\n",
    "p = 12 # AR(p) model order\n",
    "f_min = 80 # minimum frequency for pitch detection (Hz)\n",
    "f_max = 400 # maximum frequency for pitch detection Hz)\n",
    "pitch_detection_threshold = 0.5 # threshold for pitch detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 279/279 [00:00<00:00, 2906.03it/s]\n"
     ]
    }
   ],
   "source": [
    "## synthesis\n",
    "\n",
    "# load audio\n",
    "x = librosa.load('audio.wav', sr=sampling_rate)[0]\n",
    "\n",
    "# pre-emphasis filtering\n",
    "x = scipy.signal.lfilter([1, -0.98], [1], x)\n",
    "\n",
    "# handy variables\n",
    "n_frame = math.floor(len(x) / (sampling_rate * frame_duration * (1 - overlap_ratio))) - 1 # discard last frame\n",
    "frame_length = int(sampling_rate * frame_duration)\n",
    "min_period = math.floor(sampling_rate / f_max)\n",
    "max_period = math.ceil(sampling_rate / f_min)\n",
    "synthesis = np.zeros(len(x))\n",
    "log_dict = defaultdict(list)\n",
    "\n",
    "# loop over frames\n",
    "for frame_index in tqdm(range(n_frame)):\n",
    "\n",
    "    # extract frame\n",
    "    start = int(frame_index * sampling_rate * frame_duration * (1 - overlap_ratio))\n",
    "    end = start + frame_length\n",
    "    frame = x[start:end]\n",
    "\n",
    "    # detect pitch\n",
    "    auto_covariance = np.fft.irfft(np.abs(np.fft.rfft(frame - np.mean(frame), (2*frame_length-1)))**2 / frame_length)[:frame_length]\n",
    "    pitch, max_value = auto_covariance[min_period:].argmax() + min_period, auto_covariance[min_period:].max() / auto_covariance[0]\n",
    "    if (pitch > max_period) or (max_value * frame_length / (frame_length - pitch) < pitch_detection_threshold): # detect silent frame \n",
    "        pitch = 0\n",
    "    \n",
    "    # estimate AR(p) coefficients\n",
    "    gamma = ...\n",
    "    v = np.array([1 if k==0 else 0 for k in range(p+1)])\n",
    "    coefficient = ...\n",
    "    sigma = ...\n",
    "    coefficient = ...\n",
    "\n",
    "    # re-synthetise frame\n",
    "    if pitch == 0:\n",
    "        noise = ...\n",
    "        frame_r = scipy.signal.lfilter([1], coefficient, noise)\n",
    "    else:\n",
    "        dirac_comb = ...\n",
    "        frame_r = scipy.signal.lfilter([1], coefficient, dirac_comb)\n",
    "    frame_r = frame_r[p:]\n",
    "    normalization_factor = np.sqrt(sigma / np.var(frame_r))\n",
    "    frame_r = normalization_factor * frame_r[:frame_length]\n",
    "\n",
    "    # overlap-add\n",
    "    synthesis[start:end] += frame_r * np.hanning(frame_length)\n",
    "\n",
    "    # compute power spectral density (for logging purposes)\n",
    "    original_psd = ...\n",
    "    synthesis_psd = ...\n",
    "\n",
    "    # log\n",
    "    log_dict[\"frame\"].append(frame)\n",
    "    log_dict[\"pitch\"].append(pitch)\n",
    "    log_dict[\"sigma\"].append(sigma)\n",
    "    log_dict[\"coefficient\"].append(coefficient)\n",
    "    log_dict[\"frame_r\"].append(frame_r)\n",
    "    log_dict[\"original_psd\"].append(original_psd)\n",
    "    log_dict[\"synthesis_psd\"].append(synthesis_psd)\n",
    "\n",
    "# de-emphasis filtering\n",
    "synthesis = scipy.signal.lfilter([1], [1, -0.98], synthesis)\n",
    "\n",
    "# log\n",
    "sf.write('synthesis.wav', synthesis, sampling_rate)\n",
    "log_dict[\"synthesis\"].append(synthesis)\n",
    "log_dict = {k: np.array(v) for k, v in log_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot\n",
    "\n",
    "# time axis for plotting\n",
    "sample_time = np.linspace(0, len(x)/sampling_rate, len(x))\n",
    "frame_time = np.linspace(0, n_frame*frame_duration*(1-overlap_ratio), n_frame)\n",
    "\n",
    "# waveform\n",
    "plt.title(\"waveform\")\n",
    "plt.plot(sample_time, x)\n",
    "plt.show()\n",
    "\n",
    "# pitch\n",
    "plt.title(\"pitch\")\n",
    "plt.plot(frame_time, log_dict[\"pitch\"])\n",
    "plt.show()\n",
    "\n",
    "# variance\n",
    "plt.title(\"sigma\")\n",
    "plt.plot(frame_time, log_dict[\"sigma\"])\n",
    "plt.show()\n",
    "\n",
    "# power spectral density\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"estimated & theoretical spectral density\")\n",
    "y_min = min(log_dict[\"original_psd\"].min(), log_dict[\"synthesis_psd\"].min())\n",
    "y_max = max(log_dict[\"original_psd\"].max(), log_dict[\"synthesis_psd\"].max())\n",
    "def animate(i):\n",
    "    ax.cla()\n",
    "    ax.set_ylim([y_min-0.1, y_max+0.1])\n",
    "    ax.plot(log_dict[\"original_psd\"][i], label=\"original_psd\")\n",
    "    ax.plot(log_dict[\"synthesis_psd\"][i], label=\"synthesis_psd\")\n",
    "animation = matplotlib.animation.FuncAnimation(fig, animate, frames=len(log_dict[\"original_psd\"]))\n",
    "display(HTML(animation.to_jshtml()))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
